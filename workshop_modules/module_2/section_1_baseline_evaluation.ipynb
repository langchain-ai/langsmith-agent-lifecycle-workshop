{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2, Section 1: Establishing Baseline with Offline Evaluation\n",
    "\n",
    "We now have an MVP customer support agent for TechHub that can answer questions about orders, provide product information, and explain store policies. But before we can put it infront of customers, we first need build up confidence that it does the things we expect it to do.\n",
    "\n",
    "Throughout this module, we'll learn how to run offline evaluations to establish baseline performance and then systematically improve our agent with evaluation driven development (EDD).\n",
    "\n",
    "**What is offline evaluation?**\n",
    "\n",
    "Evaluation is a crucial, ongoing process that allows us to quantitatively measure how well our application is working, identify areas for improvement, and reliably evolve our system over time.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"../../images/offline_eval_process.png\">\n",
    "</div>\n",
    "\n",
    "The offline evaluation process is comprised of a few components:\n",
    "\n",
    "1. Dataset - a curated set of representative examples, where each example includes:\n",
    "    - an input to the system\n",
    "    - a ground truth (reference) output that demonstrates what the expected, high quality outcome should look like\n",
    "2. Application - the LLM system that we intend to evaluate. We feed it our example inputs, and collect the system's actual output.\n",
    "3. Evaluators - functions that quantify some aspect of performance by comparing the inputs, outputs, and reference outputs\n",
    "\n",
    "\n",
    "**What makes a good eval setup?**\n",
    "\n",
    "When beginning to create a new eval suite, it's often best to:\n",
    "\n",
    "- Gather a small set of labeled examples that are representative of your system's core functionality and scenarios it should handle.\n",
    "- Lean on domain expertise to ensure the examples are representative and accurate.\n",
    "- Select only a few, simple metrics. In practice, binary evaluation metrics force clearer thinking, more consistent labeling, and are easier/faster to interpret when analyzing and iterating on your system.\n",
    "\n",
    "Starting with a large sample and/or many, complex metrics makes it harder to inspect and deeply understand system behavior, which quickly leads to analysis paralysis.\n",
    "\n",
    "Let's see how we can perform offline evaluation on our TechHub agent in LangSmith to establish the baseline performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Curate a set of representative examples\n",
    "\n",
    "In our use case, we've teamed up with the TechHub customer support team to create 10 ground truth examples. Each example has:\n",
    "\n",
    "- **inputs**: A customers question\n",
    "- **outputs**: The expected correct answer (i.e. ground truth)\n",
    "- **metadata**: A category that the customer support team uses to bucket question types\n",
    "\n",
    "This dataset structure allows us to evaluate the \"end-to-end\" nature of our agent - commonly referred to as [final answer evaluation](https://docs.langchain.com/langsmith/evaluation-approaches#evaluating-an-agent%E2%80%99s-final-response).\n",
    "\n",
    "Let's load and explore the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "# Load the dataset from JSON\n",
    "dataset_path = Path(\"baseline_dataset.json\")\n",
    "\n",
    "with open(dataset_path, \"r\") as f:\n",
    "    examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a dataset in LangSmith\n",
    "\n",
    "Note that we convert the examples into `messages` format so that its natively stored in LangSmith with the structure needed to use these examples when invoking our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = f\"techhub-baseline-eval-{uuid.uuid4()}\"\n",
    "dataset_description = \"Representative set of customer support questions and answers curated by our support team\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=dataset_description,\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    inputs=[\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": ex[\"inputs\"][\"question\"]}]}\n",
    "        for ex in examples\n",
    "    ],\n",
    "    outputs=[\n",
    "        {\"messages\": [{\"role\": \"assistant\", \"content\": ex[\"outputs\"][\"answer\"]}]}\n",
    "        for ex in examples\n",
    "    ],\n",
    "    metadata=[ex[\"metadata\"] for ex in examples],\n",
    ")\n",
    "\n",
    "print(f\"Dataset in LangSmith: {dataset.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize the agent we want to evaluate\n",
    "\n",
    "Here we'll use the the supervisor agent with HITL verification that we built in Section 4 of Module 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from agents.supervisor_hitl_agent import create_supervisor_hitl_agent\n",
    "\n",
    "agent = create_supervisor_hitl_agent()\n",
    "\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "t = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What's your return policy for opened electronics?\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define our evaluators\n",
    "\n",
    "Evaluators are functions that score how well your application performs on a particular example.\n",
    "\n",
    "We'll start out with two simple evaluators: `correctness` and `total_tool_calls`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator #1: Correctness\n",
    "\n",
    "An evaluator that uses LLM-as-a-Judge to determine if the agent's output is \"correct\" when comparing it against the reference output (i.e. ground truth output).\n",
    "\n",
    "> Note: We manually define the LLM-as-a-Judge evaluator below for clarity, but you can achieve the same goal with fewer lines of code via our [openevals](https://github.com/langchain-ai/openevals) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.chat_models import init_chat_model\n",
    "from config import DEFAULT_MODEL\n",
    "\n",
    "\n",
    "CORRECTNESS_PROMPT = \"\"\"You are an expert data labeler evaluating model outputs for correctness.\n",
    "\n",
    "Your task is to assign a boolean score based on the following rubric:\n",
    "\n",
    "<Rubric>\n",
    "  A correct answer (True):\n",
    "  - Provides accurate and complete information\n",
    "  - Contains no factual errors\n",
    "  - Addresses all parts of the question\n",
    "  - Is logically consistent\n",
    "</Rubric>\n",
    "\n",
    "<Instructions>\n",
    "  - Carefully read the input and output\n",
    "  - Compare the output to the reference_output\n",
    "  - Check for factual accuracy and completeness\n",
    "  - Focus on correctness of information rather than style or verbosity differences\n",
    "  - Return a boolean score (True if correct, False if incorrect), not a string\n",
    "</Instructions>\n",
    "\n",
    "<Note>\n",
    "- It's ok if the ouput provides additional information that is not directly included in the reference output\n",
    "- The output is just the final output from an agent invocation, so it will not include all the intermediate steps or tool calls, this is ok.\n",
    "</Note>\n",
    "\n",
    "<input>\n",
    "{inputs}\n",
    "</input>\n",
    "\n",
    "<output>\n",
    "{outputs}\n",
    "</output>\n",
    "\n",
    "<reference_outputs>\n",
    "{reference_outputs}\n",
    "</reference_outputs>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# For structured LLM output\n",
    "class CorrectnessScore(BaseModel):\n",
    "    reasoning: str = Field(..., description=\"A concise reasoning for the score\")\n",
    "    score: bool = Field(\n",
    "        ..., description=\"True if the output is correct, False if incorrect.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Create a structured LLM\n",
    "correctness_evaluator_llm = init_chat_model(model=DEFAULT_MODEL).with_structured_output(\n",
    "    CorrectnessScore\n",
    ")\n",
    "\n",
    "\n",
    "# Define the evaluator function\n",
    "def correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Evaluate the correctness of the output against the reference output.\"\"\"\n",
    "\n",
    "    formatted_prompt = CORRECTNESS_PROMPT.format(\n",
    "        inputs=inputs, outputs=outputs, reference_outputs=reference_outputs\n",
    "    )\n",
    "\n",
    "    eval_result = correctness_evaluator_llm.invoke(formatted_prompt)\n",
    "\n",
    "    # return a dictionary with the format the evaluator expects\n",
    "    return {\n",
    "        \"key\": \"correctness\",\n",
    "        \"score\": eval_result.score,\n",
    "        \"comment\": eval_result.reasoning,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Function\n",
    "\n",
    "A target function is used to specify how `inputs` from our dataset are processed to produce `outputs` that we want to evaluate. In this case, its simply just running the `inputs` through our agent to produce a response message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "\n",
    "def target_function(inputs: dict) -> dict:\n",
    "    \"\"\"Target function that runs our agent to get outputs for evaluation.\"\"\"\n",
    "\n",
    "    thread_id = uuid.uuid4()\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    result = agent.invoke(\n",
    "        inputs,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"messages\": [{\"role\": \"assistant\", \"content\": result[\"messages\"][-1].content}]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test the `correctness` evaluator on a single example from our dataset to see how everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an example from our dataset\n",
    "example = next(\n",
    "    client.list_examples(dataset_id=dataset.id, metadata={\"example_number\": 1})\n",
    ")\n",
    "pprint(example.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the example inputs through our target function\n",
    "output = target_function(example.inputs)\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the agent output against the reference output\n",
    "correctness_score = correctness_evaluator(\n",
    "    inputs=example.inputs, outputs=output, reference_outputs=example.outputs\n",
    ")\n",
    "pprint(correctness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator #2: Total Tool Calls\n",
    "\n",
    "An evaluator that doesn't rely on a ground truth reference, but is a good thing to track as it can help reveal agent patterns and highlight inefficiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run\n",
    "\n",
    "\n",
    "def count_total_tool_calls_evaluator(run: Run) -> dict:\n",
    "    \"\"\"\n",
    "    Count total tool calls across the entire run (supervisor + sub-agents).\n",
    "\n",
    "    Returns a single 'score' metric with the total count.\n",
    "    Use for tracking efficiency: fewer calls = more efficient.\n",
    "    \"\"\"\n",
    "\n",
    "    def traverse_runs(run_obj: Run) -> int:\n",
    "        \"\"\"Recursively count all tool-type runs in the tree.\"\"\"\n",
    "        count = 0\n",
    "\n",
    "        # Count this run if it's a tool execution\n",
    "        if run_obj.run_type == \"tool\":\n",
    "            count = 1\n",
    "\n",
    "        # Recursively count child runs\n",
    "        if hasattr(run_obj, \"child_runs\") and run_obj.child_runs:\n",
    "            for child in run_obj.child_runs:\n",
    "                count += traverse_runs(child)\n",
    "\n",
    "        return count\n",
    "\n",
    "    total_tools = traverse_runs(run)\n",
    "\n",
    "    return {\"key\": \"total_tool_calls\", \"score\": total_tools}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This evaluator doesn't depend on reference outputs to produce a score, but rather uses metadata from the target function's execution on a given example. This is flexibly handled in the LangSmith SDK by passing a `Run` object as input to the evaluator.\n",
    "\n",
    "> Note: see [this docs page](https://docs.langchain.com/langsmith/code-evaluator#evaluator-args) for a list of all arguments accepted by an evaluator function\n",
    "\n",
    "Let's walk through an example to make this clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a recent sample run from our project\n",
    "runs = client.list_runs(\n",
    "    project_name=\"langsmith-agent-lifecycle-workshop\",  # Your project\n",
    "    filter=\"\"\"and(eq(is_root, true), eq(name, \"supervisor_hitl_agent\"))\"\"\",  # get a LangGraph (i.e. supervisor) run\n",
    "    limit=1,\n",
    ")\n",
    "run = next(runs)\n",
    "\n",
    "# Fetch the complete run with all children\n",
    "full_run = client.read_run(run.id, load_child_runs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can inspect the full run metadata\n",
    "# vars(full_run)\n",
    "\n",
    "# or just look at the child runs\n",
    "vars(full_run).get(\"child_runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets pass the run to our evaluator\n",
    "count_total_tool_calls_evaluator(full_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run an experiment on the full dataset\n",
    "\n",
    "Now we can use our target_function and two evaluators to programatically run an offline evaluation over each example in our dataset - this is called an \"experiment\" in LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable parallelism warnings from the tokenizers library to keep notebook output clean\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Run the experiment\n",
    "results = client.evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[correctness_evaluator, count_total_tool_calls_evaluator],\n",
    "    experiment_prefix=\"baseline-eval\",\n",
    "    description=\"Evaluate the final answer correctness and total tool calls of our agent on the baseline dataset\",\n",
    "    max_concurrency=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis in LangSmith UI\n",
    "\n",
    "Now let's analyze our baseline performance in the LangSmith UI via the link above.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
