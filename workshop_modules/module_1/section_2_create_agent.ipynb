{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1, Section 2: Building Your First Agent\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"../../images/db_agent.png\">\n",
    "</div>\n",
    "\n",
    "In Section 1, we built the tool calling loop manually. While educational, that's a lot of tedious code.\n",
    "\n",
    "In this section, we'll build our database agent using [`create_agent`](https://docs.langchain.com/oss/python/langchain/agents#agents) - a powerful abstraction that:\n",
    "- Handles the entire tool calling loop automatically\n",
    "- Adds conversational (i.e. short-term) memory out of the box\n",
    "- Uses threads for managing separate conversations\n",
    "- Supports streaming for better UX\n",
    "\n",
    "By the end, you'll see how `create_agent` replaces ~50 lines of manual code with ~5 lines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Tools\n",
    "\n",
    "**Note on Refactoring:** In Section 1, we defined tools inline for learning purposes. Now that you understand how tools work, we've moved them to the shared `tools/` directory for reuse across multiple sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our shared database tools\n",
    "from tools.database import (\n",
    "    get_order_status,\n",
    "    get_order_items,\n",
    "    get_product_info,\n",
    "    get_order_item_price,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Your First Agent\n",
    "\n",
    "Let's replace all that manual loop code with the `create_agent` abstraction which will run the loop for us:\n",
    "1. Model decides which tool to call (if any)\n",
    "2. Tool gets executed\n",
    "3. Result goes back to model\n",
    "4. Repeat until task is complete\n",
    "\n",
    "The prebuilt agent handles running the loop described above - you just specify the system prompt and tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from config import DEFAULT_MODEL\n",
    "\n",
    "# Initialize model (using workshop default from config.py)\n",
    "llm = init_chat_model(DEFAULT_MODEL)\n",
    "\n",
    "# Create agent - THIS REPLACES ALL THE MANUAL LOOP CODE!\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[get_order_status, get_order_items, get_product_info, get_order_item_price],\n",
    "    name=\"db_agent\",\n",
    "    system_prompt=\"You are a helpful customer support assistant for TechHub.\",\n",
    ")\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the benefits:\n",
    "  - Automatic tool calling loop\n",
    "  - No manual message management\n",
    "  - Just ~5 lines of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it with the same query as Section 1.\n",
    "\n",
    "> Note that we're now passing `messages` formatted as a list of dictionaries instead of the explicit `HumanMessage` class from the last section. Either work, and you can learn more [here](https://docs.langchain.com/oss/python/langchain/messages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What's the status of order ORD-2024-0123?\"}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "for message in result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** This works great for single queries! But what happens when we try to ask a follow-up question..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a follow-up that references the previous query\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"When was it shipped?\"}]}\n",
    ")\n",
    "\n",
    "result[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️  The agent doesn't remember the previous order! We need memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Short-term Memory with Checkpointer\n",
    "\n",
    "Right now, each agent invocation is independent. Let's add **short-term memory** so the agent can maintain context across multi-turn conversations.\n",
    "\n",
    "LangChain is built on LangGraph, which uses **checkpointers** to save and restore state. We get this out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Add checkpointer for memory\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "agent_with_memory = create_agent(\n",
    "    model=llm,\n",
    "    tools=[get_order_status, get_order_items, get_product_info, get_order_item_price],\n",
    "    system_prompt=\"You are a helpful customer support assistant for TechHub.\",\n",
    "    name=\"db_agent_with_memory\",\n",
    "    checkpointer=checkpointer,  # This enables memory!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use checkpointers, we need to pass a `thread_id` when invoking the agent. The `thread_id` acts as a unique identifier for each conversation, allowing the agent to keep separate histories for different users or sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Create a thread for this conversation - common practice to use a uuid\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# Turn 1: Ask about an order\n",
    "print(\"[Turn 1]\")\n",
    "result = agent_with_memory.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What's the status of order ORD-2024-0123?\"}\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "result[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Turn 2: Follow-up question (references \"it\" from previous turn)\n",
    "print(\"\\n[Turn 2]\")\n",
    "result = agent_with_memory.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"When was it shipped?\"}]},\n",
    "    config=config,  # same thread_id\n",
    ")\n",
    "result[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ The agent now remembers details from earlier in the conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Thread Separation\n",
    "\n",
    "Different `thread_id`s create separate conversations with isolated memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread 1: Customer A\n",
    "config_user1 = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "result = agent_with_memory.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What's the status of order ORD-2024-0123?\"}\n",
    "        ]\n",
    "    },\n",
    "    config=config_user1,\n",
    ")\n",
    "print(f\"[Customer A]: {result['messages'][-1].content}...\\n\")\n",
    "\n",
    "# Thread 2: Customer B (different conversation)\n",
    "config_user2 = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "result = agent_with_memory.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"How much is the MacBook Air?\"}]},\n",
    "    config=config_user2,\n",
    ")\n",
    "print(f\"[Customer B]: {result['messages'][-1].content}...\\n\")\n",
    "\n",
    "# Back to Thread 1: Memory is preserved\n",
    "result = agent_with_memory.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"When was it shipped?\"}]},\n",
    "    config=config_user1,\n",
    ")\n",
    "print(f\"[Customer A follow-up]: {result['messages'][-1].content}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaway:\n",
    "- Checkpointers enable memory across interactions\n",
    "- Thread IDs separate different conversations\n",
    "- State persists automatically - no manual state management needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming Responses for Better UX\n",
    "\n",
    "LLMs can take a while to respond.\n",
    "\n",
    "**Streaming** shows progress in real-time, dramatically improving user experience. To stream responses token-by-token - just change `.invoke()` to `.stream()`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Agent Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream agent progress with stream_mode=\"updates\"\n",
    "print(\"Streaming agent steps:\\n\")\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What's the status of order ORD-2024-0125?\"}\n",
    "        ]\n",
    "    },\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    for node_name, data in chunk.items():\n",
    "        print(f\"Step: {node_name}\")\n",
    "        if \"messages\" in data:\n",
    "            message = data[\"messages\"][-1]\n",
    "            if hasattr(message, \"tool_calls\") and message.tool_calls:\n",
    "                print(f\"   Tool call: {message.tool_calls[0]['name']}\")\n",
    "            elif hasattr(message, \"content\"):\n",
    "                print(f\"   Content: {message.content}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming LLM Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Streaming response:\\n\")\n",
    "\n",
    "for message_chunk, metadata in agent.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"What's the status of order ORD-2024-0125?\"}\n",
    "        ]\n",
    "    },\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    # only print content from the model node\n",
    "    if metadata.get(\"langgraph_node\") == \"model\":\n",
    "        # Get text from content blocks\n",
    "        for block in message_chunk.content_blocks:\n",
    "            if block.get(\"type\") == \"text\" and block.get(\"text\"):\n",
    "                print(block.get(\"text\"), end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Takeaway:\n",
    "- `stream_mode=\"updates\"` - See each agent step (useful for debugging)\n",
    "- `stream_mode=\"messages\"` - Stream LLM tokens (ChatGPT-like UX)\n",
    "- Streaming is built-in - no extra setup required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** You can control what types of information are streamed (e.g., messages, tool calls, etc.) by setting the `stream_mode` parameter. This allows fine-grained control over what you receive in real-time. See the [LangGraph streaming docs](https://docs.langchain.com/oss/python/langgraph/streaming) for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Review the agent traces in LangSmith!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
